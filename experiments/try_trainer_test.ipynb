{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DistilBERT Sequence Classification Trainer\n",
        "\n",
        "This notebook demonstrates how to fine-tune a DistilBERT model for sequence classification using the Hugging Face Transformers library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install accelerate package - required for Trainer\n",
        "%pip install \"accelerate>=0.26.0\" --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import json \n",
        "import torch \n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import os\n",
        "import numpy as np \n",
        "import evaluate \n",
        "from transformers.trainer import Trainer\n",
        "from transformers.training_args import TrainingArguments\n",
        "\n",
        "# Disable wandb logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare model and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", \n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     \"roberta-base\", \n",
        "#     num_labels=2\n",
        "# )\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random \n",
        "\n",
        "random.seed(2025)\n",
        "\n",
        "# Load dataset\n",
        "with open(\"../data/classification/incorrect/train_v1.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open(\"../data/classification/incorrect/train_v2.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data_v2 = json.load(f)\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = {\n",
        "    \"label\": [],\n",
        "    \"text\": []\n",
        "}\n",
        "for dialog in data: \n",
        "    for i, sentence in enumerate(dialog): \n",
        "        if i == 0: \n",
        "            dataset[\"label\"].append(1)\n",
        "            dataset[\"text\"].append(sentence)\n",
        "        else: \n",
        "            if sentence == None : \n",
        "                continue \n",
        "            else: \n",
        "                dataset[\"label\"].append(0)\n",
        "                dataset[\"text\"].append(sentence)\n",
        "\n",
        "for dialog in data_v2: \n",
        "    for i, sentence in enumerate(dialog): \n",
        "        if i == 0: \n",
        "            dataset[\"label\"].append(1)\n",
        "            dataset[\"text\"].append(sentence)\n",
        "        else: \n",
        "            if sentence == None : \n",
        "                continue \n",
        "            else: \n",
        "                dataset[\"label\"].append(0)\n",
        "                dataset[\"text\"].append(sentence)\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_dict(dataset)\n",
        "\n",
        "# Split dataset into train and test\n",
        "train_size = int(0.9 * len(dataset))\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "test_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Print ratio classification\n",
        "print(f\"Ratio classification: {train_dataset['label'].count(1) / len(train_dataset)}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize dataset\n",
        "def tokenize_function(examples): \n",
        "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "    tokens[\"labels\"] = examples[\"label\"]\n",
        "    return tokens\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define metrics and compute_metrics function\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred): \n",
        "    logits, labels = eval_pred \n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
        "    # Return a dictionary with all metrics\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"] if accuracy is not None else None,\n",
        "        \"f1\": f1[\"f1\"] if f1 is not None else None\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay = 0.1,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4090' max='4090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4090/4090 06:04, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.502500</td>\n",
              "      <td>0.421037</td>\n",
              "      <td>0.780756</td>\n",
              "      <td>0.595691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>0.408650</td>\n",
              "      <td>0.778007</td>\n",
              "      <td>0.619552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.375500</td>\n",
              "      <td>0.408358</td>\n",
              "      <td>0.789003</td>\n",
              "      <td>0.640094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.444314</td>\n",
              "      <td>0.779381</td>\n",
              "      <td>0.637288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.283200</td>\n",
              "      <td>0.467105</td>\n",
              "      <td>0.779381</td>\n",
              "      <td>0.631458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4090, training_loss=0.37012760423506386, metrics={'train_runtime': 364.7584, 'train_samples_per_second': 179.393, 'train_steps_per_second': 11.213, 'total_flos': 8668004231055360.0, 'train_loss': 0.37012760423506386, 'epoch': 5.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4671052396297455, 'eval_accuracy': 0.7793814432989691, 'eval_f1': 0.6314580941446614, 'eval_runtime': 2.4071, 'eval_samples_per_second': 604.451, 'eval_steps_per_second': 37.804, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(evaluation_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded test data with 1465 dialogs\n",
            "Processed 5782 test sentences\n",
            "Label distribution - Correct (1): 1465, Incorrect (0): 4317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 5782/5782 [00:00<00:00, 16295.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test dataset: Dataset({\n",
            "    features: ['text', 'label', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 5782\n",
            "})\n",
            "\n",
            "==================================================\n",
            "RUNNING MODEL PREDICTIONS...\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions shape: (5782, 2)\n",
            "Probabilities shape: (5782, 2)\n",
            "Predicted classes shape: (5782,)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load test data\n",
        "test_file_path = \"../data/classification/incorrect/train_v2.json\"\n",
        "with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded test data with {len(test_data)} dialogs\")\n",
        "\n",
        "# Process test data - create labels based on position (first sentence = correct=1, others = incorrect=0)\n",
        "test_sentences = []\n",
        "test_labels = []\n",
        "\n",
        "for dialog in test_data:\n",
        "    for i, sentence in enumerate(dialog):\n",
        "        if sentence is None:\n",
        "            continue\n",
        "        \n",
        "        # Handle different data structures\n",
        "        if isinstance(sentence, list):\n",
        "            # If it's a list of tokens, join them\n",
        "            if all(isinstance(token, list) for token in sentence):\n",
        "                # Token-level structure [[word, tag, ...], ...]\n",
        "                text = \" \".join([token[0] for token in sentence if len(token) > 0])\n",
        "            else:\n",
        "                # Simple list of words\n",
        "                text = \" \".join(sentence)\n",
        "        elif isinstance(sentence, str):\n",
        "            text = sentence\n",
        "        elif isinstance(sentence, dict) and \"text\" in sentence:\n",
        "            text = sentence[\"text\"]\n",
        "        else:\n",
        "            continue\n",
        "            \n",
        "        if text.strip():  # Only add non-empty sentences\n",
        "            test_sentences.append(text.strip())\n",
        "            # First sentence in dialog = correct (label 1), others = incorrect (label 0)\n",
        "            test_labels.append(1 if i == 0 else 0)\n",
        "\n",
        "print(f\"Processed {len(test_sentences)} test sentences\")\n",
        "print(f\"Label distribution - Correct (1): {test_labels.count(1)}, Incorrect (0): {test_labels.count(0)}\")\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = Dataset.from_dict({\n",
        "    \"text\": test_sentences,\n",
        "    \"label\": test_labels\n",
        "})\n",
        "\n",
        "# Tokenize test dataset\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "print(f\"Test dataset: {tokenized_test_dataset}\")\n",
        "\n",
        "# Run predictions\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RUNNING MODEL PREDICTIONS...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "predictions = trainer.predict(tokenized_test_dataset)\n",
        "y_pred_logits = predictions.predictions\n",
        "y_true = test_labels\n",
        "\n",
        "# Convert logits to probabilities and predicted classes\n",
        "y_pred_probs = F.softmax(torch.tensor(y_pred_logits), dim=-1).numpy()\n",
        "y_pred_classes = np.argmax(y_pred_logits, axis=-1)\n",
        "\n",
        "print(f\"Predictions shape: {y_pred_logits.shape}\")\n",
        "print(f\"Probabilities shape: {y_pred_probs.shape}\")\n",
        "print(f\"Predicted classes shape: {y_pred_classes.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "OVERALL MODEL PERFORMANCE\n",
            "============================================================\n",
            "Accuracy: 0.8622\n",
            "Precision: 0.8871\n",
            "Recall: 0.8622\n",
            "F1-Score: 0.8679\n",
            "\n",
            "============================================================\n",
            "DETAILED CLASSIFICATION REPORT\n",
            "============================================================\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Incorrect (0)       0.96      0.85      0.90      4317\n",
            "  Correct (1)       0.67      0.90      0.77      1465\n",
            "\n",
            "     accuracy                           0.86      5782\n",
            "    macro avg       0.82      0.87      0.83      5782\n",
            " weighted avg       0.89      0.86      0.87      5782\n",
            "\n",
            "\n",
            "============================================================\n",
            "CONFUSION MATRIX\n",
            "============================================================\n",
            "           Predicted\n",
            "         0 (Inc)  1 (Cor)\n",
            "Actual 0    3670     647\n",
            "Actual 1     150    1315\n",
            "\n",
            "True Negatives (correctly identified incorrect): 3670\n",
            "False Positives (incorrectly identified as correct): 647\n",
            "False Negatives (incorrectly identified as incorrect): 150\n",
            "True Positives (correctly identified correct): 1315\n",
            "\n",
            "============================================================\n",
            "MODEL PREDICTIONS ANALYSIS\n",
            "============================================================\n",
            "Average prediction confidence: 0.8886\n",
            "Min prediction confidence: 0.5003\n",
            "Max prediction confidence: 0.9991\n",
            "\n",
            "Correct predictions: 4985 / 5782 (0.862)\n",
            "Average confidence for correct predictions: 0.9140\n",
            "Average confidence for incorrect predictions: 0.7300\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# COMPREHENSIVE MODEL EVALUATION\n",
        "# ========================================\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"OVERALL MODEL PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true, y_pred_classes, target_names=['Incorrect (0)', 'Correct (1)']))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\"*60)\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "print(\"           Predicted\")\n",
        "print(\"         0 (Inc)  1 (Cor)\")\n",
        "print(f\"Actual 0  {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
        "print(f\"Actual 1  {cm[1,0]:6d}  {cm[1,1]:6d}\")\n",
        "\n",
        "# Calculate per-class metrics\n",
        "print(f\"\\nTrue Negatives (correctly identified incorrect): {cm[0,0]}\")\n",
        "print(f\"False Positives (incorrectly identified as correct): {cm[0,1]}\")\n",
        "print(f\"False Negatives (incorrectly identified as incorrect): {cm[1,0]}\")\n",
        "print(f\"True Positives (correctly identified correct): {cm[1,1]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PREDICTIONS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get prediction confidence distribution\n",
        "max_probs = np.max(y_pred_probs, axis=1)\n",
        "print(f\"Average prediction confidence: {np.mean(max_probs):.4f}\")\n",
        "print(f\"Min prediction confidence: {np.min(max_probs):.4f}\")\n",
        "print(f\"Max prediction confidence: {np.max(max_probs):.4f}\")\n",
        "\n",
        "# Show confidence distribution by class\n",
        "correct_predictions = y_pred_classes == y_true\n",
        "print(f\"\\nCorrect predictions: {np.sum(correct_predictions)} / {len(y_true)} ({np.mean(correct_predictions):.3f})\")\n",
        "print(f\"Average confidence for correct predictions: {np.mean(max_probs[correct_predictions]):.4f}\")\n",
        "print(f\"Average confidence for incorrect predictions: {np.mean(max_probs[~correct_predictions]):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE PREDICTIONS (Text + Predictions)\n",
            "================================================================================\n",
            "\n",
            "CORRECTLY PREDICTED EXAMPLES\n",
            "----------------------------\n",
            "\n",
            "Example 1: ✓ CORRECT\n",
            "Text: \"i've got some bad news about the bike you lent me.\"\n",
            "True Label: 1 (Correct English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.799]\n",
            "\n",
            "Example 2: ✓ CORRECT\n",
            "Text: \"i've got some bad news about the bike lent me.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.947]\n",
            "\n",
            "Example 3: ✓ CORRECT\n",
            "Text: \"i've some bad news about the bike you got lent me.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.999]\n",
            "\n",
            "Example 4: ✓ CORRECT\n",
            "Text: \"i've got any bad news about the bike you lent me.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.524]\n",
            "\n",
            "Example 5: ✓ CORRECT\n",
            "Text: \"what's that?\"\n",
            "True Label: 1 (Correct English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.836]\n",
            "\n",
            "INCORRECTLY PREDICTED EXAMPLES\n",
            "------------------------------\n",
            "\n",
            "Example 12: ✗ WRONG\n",
            "Text: \"he fell on the way to school, and your bike got scratched. i'm really sorry.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.782]\n",
            "\n",
            "Example 20: ✗ WRONG\n",
            "Text: \"yes, thank you.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.853]\n",
            "\n",
            "Example 24: ✗ WRONG\n",
            "Text: \"that's the most unimportant thing.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.830]\n",
            "\n",
            "Example 38: ✗ WRONG\n",
            "Text: \"maybe, but i have fallen a couple of times and it's been hit once or twice as well.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.697]\n",
            "\n",
            "Example 56: ✗ WRONG\n",
            "Text: \"oh, the day after yesterday is christmas.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.737]\n",
            "\n",
            "HIGH CONFIDENCE PREDICTIONS (>90%)\n",
            "----------------------------------\n",
            "\n",
            "Example 2: ✓ CORRECT\n",
            "Text: \"i've got some bad news about the bike lent me.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.947]\n",
            "\n",
            "Example 3: ✓ CORRECT\n",
            "Text: \"i've some bad news about the bike you got lent me.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.999]\n",
            "\n",
            "Example 6: ✓ CORRECT\n",
            "Text: \"that?\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.998]\n",
            "\n",
            "Example 7: ✓ CORRECT\n",
            "Text: \"that? what's\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.999]\n",
            "\n",
            "Example 10: ✓ CORRECT\n",
            "Text: \"i fell on the way to and your bike got scratched. i'm really sorry.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.996]\n",
            "\n",
            "LOW CONFIDENCE PREDICTIONS (<70%)\n",
            "---------------------------------\n",
            "\n",
            "Example 4: ✓ CORRECT\n",
            "Text: \"i've got any bad news about the bike you lent me.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.524]\n",
            "\n",
            "Example 28: ✓ CORRECT\n",
            "Text: \"it's cruel of you to say. i feel a little stupid.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.636]\n",
            "\n",
            "Example 34: ✓ CORRECT\n",
            "Text: \"you lent me the bike, it looked brand new, almost anyway.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 0 (Incorrect English) [Confidence: 0.574]\n",
            "\n",
            "Example 37: ✓ CORRECT\n",
            "Text: \"maybe, but really i have fallen a couple of times and it's been hit once or twice as well.\"\n",
            "True Label: 1 (Correct English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.680]\n",
            "\n",
            "Example 38: ✗ WRONG\n",
            "Text: \"maybe, but i have fallen a couple of times and it's been hit once or twice as well.\"\n",
            "True Label: 0 (Incorrect English)\n",
            "Predicted: 1 (Correct English) [Confidence: 0.697]\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# EXAMPLE PREDICTIONS WITH ACTUAL TEXT\n",
        "# ========================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXAMPLE PREDICTIONS (Text + Predictions)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Show examples of correct and incorrect predictions\n",
        "def show_prediction_examples(texts, true_labels, pred_classes, pred_probs, title, indices=None, n_examples=10):\n",
        "    print(f\"\\n{title}\")\n",
        "    print(\"-\" * len(title))\n",
        "    \n",
        "    if indices is None:\n",
        "        indices = range(min(n_examples, len(texts)))\n",
        "    \n",
        "    for i in indices[:n_examples]:\n",
        "        text = texts[i]\n",
        "        true_label = true_labels[i]\n",
        "        pred_label = pred_classes[i]\n",
        "        confidence = pred_probs[i][pred_label]\n",
        "        \n",
        "        status = \"✓ CORRECT\" if true_label == pred_label else \"✗ WRONG\"\n",
        "        true_meaning = \"Correct English\" if true_label == 1 else \"Incorrect English\"\n",
        "        pred_meaning = \"Correct English\" if pred_label == 1 else \"Incorrect English\"\n",
        "        \n",
        "        print(f\"\\nExample {i+1}: {status}\")\n",
        "        print(f\"Text: \\\"{text[:100]}{'...' if len(text) > 100 else ''}\\\"\")\n",
        "        print(f\"True Label: {true_label} ({true_meaning})\")\n",
        "        print(f\"Predicted: {pred_label} ({pred_meaning}) [Confidence: {confidence:.3f}]\")\n",
        "\n",
        "# Show correct predictions\n",
        "correct_indices = np.where(y_pred_classes == y_true)[0]\n",
        "show_prediction_examples(test_sentences, y_true, y_pred_classes, y_pred_probs, \n",
        "                        \"CORRECTLY PREDICTED EXAMPLES\", correct_indices, 5)\n",
        "\n",
        "# Show incorrect predictions  \n",
        "incorrect_indices = np.where(y_pred_classes != y_true)[0]\n",
        "show_prediction_examples(test_sentences, y_true, y_pred_classes, y_pred_probs,\n",
        "                        \"INCORRECTLY PREDICTED EXAMPLES\", incorrect_indices, 5)\n",
        "\n",
        "# Show high confidence predictions\n",
        "high_conf_indices = np.where(np.max(y_pred_probs, axis=1) > 0.9)[0]\n",
        "show_prediction_examples(test_sentences, y_true, y_pred_classes, y_pred_probs,\n",
        "                        \"HIGH CONFIDENCE PREDICTIONS (>90%)\", high_conf_indices, 5)\n",
        "\n",
        "# Show low confidence predictions  \n",
        "low_conf_indices = np.where(np.max(y_pred_probs, axis=1) < 0.7)[0]\n",
        "show_prediction_examples(test_sentences, y_true, y_pred_classes, y_pred_probs,\n",
        "                        \"LOW CONFIDENCE PREDICTIONS (<70%)\", low_conf_indices, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING SINGLE SENTENCE PREDICTIONS\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Text: \"I am going to the store.\"\n",
            "Prediction: Correct English (Class: 1)\n",
            "Confidence: 0.820\n",
            "Probability Correct: 0.820\n",
            "Probability Incorrect: 0.180\n",
            "\n",
            "Example 2:\n",
            "Text: \"I is going to the store.\"\n",
            "Prediction: Incorrect English (Class: 0)\n",
            "Confidence: 0.998\n",
            "Probability Correct: 0.002\n",
            "Probability Incorrect: 0.998\n",
            "\n",
            "Example 3:\n",
            "Text: \"She writes beautiful poems.\"\n",
            "Prediction: Correct English (Class: 1)\n",
            "Confidence: 0.601\n",
            "Probability Correct: 0.601\n",
            "Probability Incorrect: 0.399\n",
            "\n",
            "Example 4:\n",
            "Text: \"She write beautiful poems.\"\n",
            "Prediction: Incorrect English (Class: 0)\n",
            "Confidence: 0.999\n",
            "Probability Correct: 0.001\n",
            "Probability Incorrect: 0.999\n",
            "\n",
            "Example 5:\n",
            "Text: \"The cat is sleeping on the sofa.\"\n",
            "Prediction: Incorrect English (Class: 0)\n",
            "Confidence: 0.975\n",
            "Probability Correct: 0.025\n",
            "Probability Incorrect: 0.975\n",
            "\n",
            "Example 6:\n",
            "Text: \"The cat are sleeping on the sofa.\"\n",
            "Prediction: Incorrect English (Class: 0)\n",
            "Confidence: 0.998\n",
            "Probability Correct: 0.002\n",
            "Probability Incorrect: 0.998\n",
            "\n",
            "Example 7:\n",
            "Text: \"How are you doing today?\"\n",
            "Prediction: Correct English (Class: 1)\n",
            "Confidence: 0.712\n",
            "Probability Correct: 0.712\n",
            "Probability Incorrect: 0.288\n",
            "\n",
            "Example 8:\n",
            "Text: \"How is you doing today?\"\n",
            "Prediction: Correct English (Class: 1)\n",
            "Confidence: 0.777\n",
            "Probability Correct: 0.777\n",
            "Probability Incorrect: 0.223\n",
            "\n",
            "================================================================================\n",
            "You can now use the predict_sentence_correctness() function\n",
            "to test any sentence! Example:\n",
            "result = predict_sentence_correctness(\"Your sentence here\")\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# INTERACTIVE INFERENCE FUNCTION\n",
        "# ========================================\n",
        "\n",
        "def predict_sentence_correctness(text, model=model, tokenizer=tokenizer):\n",
        "    \"\"\"\n",
        "    Predict whether a sentence is grammatically correct or incorrect.\n",
        "\n",
        "    Args:\n",
        "        text (str): The sentence to evaluate\n",
        "        model: The trained model\n",
        "        tokenizer: The tokenizer\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing prediction, confidence, and probabilities\n",
        "    \"\"\"\n",
        "    # Ensure model and tensors are on the same device\n",
        "    device = next(model.parameters()).device\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "        confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "    # Interpret results\n",
        "    prediction = \"Correct English\" if predicted_class == 1 else \"Incorrect English\"\n",
        "    prob_correct = probabilities[0][1].item()\n",
        "    prob_incorrect = probabilities[0][0].item()\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"prediction\": prediction,\n",
        "        \"predicted_class\": predicted_class,\n",
        "        \"confidence\": confidence,\n",
        "        \"prob_correct\": prob_correct,\n",
        "        \"prob_incorrect\": prob_incorrect\n",
        "    }\n",
        "\n",
        "# Test the function with some example sentences\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING SINGLE SENTENCE PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_examples = [\n",
        "    \"I am going to the store.\",           # Correct\n",
        "    \"I is going to the store.\",           # Incorrect (subject-verb disagreement)\n",
        "    \"She writes beautiful poems.\",        # Correct  \n",
        "    \"She write beautiful poems.\",         # Incorrect (subject-verb disagreement)\n",
        "    \"The cat is sleeping on the sofa.\",   # Correct\n",
        "    \"The cat are sleeping on the sofa.\",  # Incorrect (subject-verb disagreement)\n",
        "    \"How are you doing today?\",           # Correct\n",
        "    \"How is you doing today?\",            # Incorrect\n",
        "]\n",
        "\n",
        "for i, text in enumerate(test_examples, 1):\n",
        "    result = predict_sentence_correctness(text)\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Text: \\\"{result['text']}\\\"\")\n",
        "    print(f\"Prediction: {result['prediction']} (Class: {result['predicted_class']})\")\n",
        "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"Probability Correct: {result['prob_correct']:.3f}\")\n",
        "    print(f\"Probability Incorrect: {result['prob_incorrect']:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"You can now use the predict_sentence_correctness() function\")\n",
        "print(\"to test any sentence! Example:\")\n",
        "print('result = predict_sentence_correctness(\"Your sentence here\")')\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hoanglv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
